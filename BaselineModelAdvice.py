'''
This file describes our re-implementation of the best model proposed by Bisk et. al in their work "Natural Language Communication with Robots." We then add our system for providing advice to the agent in this file. 

This code handles restrictive advice only. You must have pre-trained a model to understand the advice (in the file PreTrainedModel.py) first. That model will then be loaded into the appropriate parts of this model to understand the advice text, and then you can train this model end-to-end.

If you want to use self-generated advice instead of always accurate advice in this model, you must also generate the advice using the InputSpecificSelfGeneratedAdvice.py file. You must then save those into a numpy array (which that file does). Then, set the parameter FLAGS.self_generated_advice to True. As this code release only has input-specific self-generated advice, you must train the pre-trained model in PreTrainedModel.py to understand all the fine-grained advice regions.

The code flows top to down. The only variables which must be changed are at the top. You must define where the data is at, where the model must be saved, and what mode you are running in. 
'''

from __future__ import division
import os,random,sys
from absl import flags
sys.path.append(".")

## Model Imports
import tensorflow as tf
import numpy as np
np.set_printoptions(threshold=np.nan)
from TFLibraries.Layer import Layers
Layer = Layers()
import pandas as pandas
import math
import time
import nltk
from nltk.tokenize import word_tokenize

random.seed(20160427)

flags.DEFINE_string("model_save_path", default="savedModels/restrictive_advice/model.ckpt", help='File to save the entire model')
flags.DEFINE_string("trained_model_save_path", default="savedModels/pre_trained_advice_advice/model.ckpt", help='Where the pre-trained model is saved')
flags.DEFINE_string("train_file", default="data/STxyz_Blank/Train.mat", help='Where the training data is stored.')
flags.DEFINE_string("dev_file", default="data/STxyz_Blank/Dev.mat", help='Where the dev data is stored.')
flags.DEFINE_string("test_file", default="data/STxyz_Blank/Test.mat", help='Where the test data is stored.')
flags.DEFINE_string("advice_tokens_save_file", default="saved_tokens/tokens.npy", help='Where the tokens for the advice (from the pre-trained model) are stored.')

# for input-specific self-generated advice, you must first run this model to generate the coordinate predictions, which the code will then generate the advice based on. For this, set FLAGS.generate_advice to True. The model will then save the advice in a file called test_advice.npy.
# then, set FLAGS.self_generated_advice to True, and the advice will be loaded from that file at test time.
flags.DEFINE_string("test_advice_save_file", default="test_advice.npy", help='Where the self-generated advice is stored. It is generated by running this model with FLAGS.generate_advice set to True.')
flags.DEFINE_bool("self_generated_advice", default=False, help='True if running input-specific self_generated_advice. If so, FLAGS.test_advice_save_file must be set.')
flags.DEFINE_bool("generate_advice", default=False, help='True if you want to generate input-specific self-generated advice so you use it with FLAGS.self_generated_advice set to True')

flags.DEFINE_integer("target_output", default=1, help="1 if prediction source coordinates, else 2 for target coordinates")
flags.DEFINE_integer("hidden_layer_size", default=256, help="Size of the hidden FC layer")
flags.DEFINE_integer("word_embedding_size", default=256, help="Size of the word embeddings")
flags.DEFINE_integer("advice_embedding_size", default=100, help="Size of the advice embeddings. Must match the pre-trained model.")
flags.DEFINE_integer("advice_hidden_layer_size", default=256, help="Size of the advice hidden FC layer. Must match the pre-trained model.")
flags.DEFINE_integer("output_hidden_layer_size", default=256, help="Size of the layer that connects advice and instruction.")
flags.DEFINE_integer("fc_column_size", default=100, help="Size of the fully connected layer from the pre-trained model that takes the advice LSTM input. Must match the pre-trained model.")
flags.DEFINE_integer("epochs", default=90, help="How many FLAGS.epochs to run for.")
flags.DEFINE_float("learning_rate", default=0.001, help="Learning Rate.")
flags.DEFINE_float("gradient_clip_threshold", default=5.0, help="Threshold for gradient clipping.")
flags.DEFINE_float("dropout", default=0.5, help="How much dropout to use at training time.")
flags.DEFINE_integer("xvocab", default=0, help="Starts at 0 and is computed based on the data.")
flags.DEFINE_integer("maxlength", default=105, help="Max length of the sentence.")
flags.DEFINE_integer("world_length", default=20, help="Length of the world grid. Is based on the data.")
flags.DEFINE_integer("world_size", default=3, help="Length of the world size. Is based on the data.")
flags.DEFINE_integer("batch_size", default=9, help="batch size")
flags.DEFINE_integer("ndirs", default=9, help="Number of directions to predict. 9 since we are predicting 9 batches.")
flags.DEFINE_integer("nblocks", default=20, help="How many blocks to predict. Based on world length")
flags.DEFINE_integer("ndims", default=3, help="How many dimensions to predict. 3 since there are 3 coordinates.")
flags.DEFINE_integer("maxadvicelength", default=40, help="Maximum length of the advice text.")
flags.DEFINE_integer("num_layers", default=1, help="How many layers are in the LSTM.")
flags.DEFINE_integer("random_seed_value", default=20160501, help="Random seed.")

FLAGS = flags.FLAGS
FLAGS(sys.argv)

tf.set_random_seed(FLAGS.random_seed_value)

# where the tokens from the pre-trained model are saved
advice_tokens_dictionary = np.load(FLAGS.advice_tokens_save_file).item()

# dictionaries to store all the dta
training = {}
training_labels = {}
training_lens = {}
development = {}
development_labels = {}
development_lens = {}
testing = {}
testing_labels = {}
testing_lens = {}

# the advice we will provide to the system. In this case we are simulating the advice by filling regions as placeholders into pre-defined sentences. In the future, this advice could be gotten from the human operator.
advice_list = ["The block's %s and %s location is in the %s",
            "The box is in the %s",
            "In the %s"]

# if target = 1, range = 1 to 3
range_start = 0
range_end = 3
if FLAGS.target_output == 2:
    range_start = 3
    range_end = 6

# Load the Data
print("Reading the data files...")
# A minibatch consists of a target location, a world, and 9 sentences that share the same target/world.
fileList = [FLAGS.train_file, FLAGS.dev_file, FLAGS.test_file]
# this list will hold train, dev, test files and all of their individual minibatches
all_data = []

def determineSpatial(x_coordinate, y_coordinate):
    ''' determine which quadrant the coordinate is.'''

    # 4 quadrants: upper left, lower left, upper right, lower right
    # upper left
    if x_coordinate < 0 and y_coordinate > 0:
        spatial_number = 0
    elif x_coordinate < 0 and y_coordinate <= 0:
        # lower left
        spatial_number = 1
    elif x_coordinate >= 0 and y_coordinate > 0:
        # upper right
        spatial_number = 2
    elif x_coordinate >= 0 and y_coordinate <= 0:
        # lower right
        spatial_number = 3

    return spatial_number

def determine_self_generated_spatial_accurate(x_coordinate, z_coordinate, true_x_coordinate, true_z_coordinate):
    '''determine if the predicted input specific self-generated advice was accurate or not. Useful to compute the accuracy of this.'''

    # as we go along, keep a counter to determine which region we are in
    x_region_counter = 0
    z_region_counter = 0

    # make this region my center
    x_left_bound = x_coordinate - 0.5
    x_right_bound = x_coordinate + .5
    z_left_bound = z_coordinate - .5
    z_right_bound = z_coordinate + .5


    x_left_bound = round(x_left_bound, 2)
    x_right_bound = round(x_right_bound, 2)
    z_left_bound = round(z_left_bound, 2)
    z_right_bound = round(z_right_bound, 2)

    # make sure our bounds do not overshoot, if they do, we can make them bigger on one side
    if x_left_bound < -1.0:
        x_left_bound = -1.0
        x_right_bound = 0.0
    if x_right_bound > 1.0:
        x_right_bound = 1.0
        x_left_bound = 0.0
    if z_left_bound < -1.0:
        z_left_bound = -1.0
        z_right_bound = 0.0
    if z_right_bound > 1.0:
        z_right_bound = 1.0
        z_left_bound = 0.0

    # check if the true coordinate falls in this region
    if not (true_x_coordinate > x_left_bound and true_x_coordinate <= x_right_bound):
        return False

    if not (true_z_coordinate > z_left_bound and true_z_coordinate <= z_right_bound):
        return False

    return True

def calculate_accuracy_self_generated(coordinates_predicted, trueCoordinate, batchsize):
    '''calculate the accuracy of input specific self-generated advice'''

    accuracyList = []
    # go through all coordinates
    for j in range(batchsize):

        if determine_self_generated_spatial_accurate(coordinates_predicted[:, j][0], coordinates_predicted[:, j][2], trueCoordinate[0][0], trueCoordinate[2][0]) == True:
            accuracyList.append(1.0)
        else:
            accuracyList.append(0.0)

    return accuracyList

def generateAdviceCenter(advice_list, random_coordinate, training=True):
    '''function to generate an advice region centered at the random input coordinate. This is for input-specific self-generated advice at training time, so the model can understand all the precise regions.'''

    distance_threshold = 0.01

    x_coordinate = random_coordinate[0]
    z_coordinate = random_coordinate[2]

    # determine the appropriate left and right bounds for this coordinate
    x_left_bound = x_coordinate - 0.5
    x_right_bound = x_coordinate + .5
    z_left_bound = z_coordinate - .5
    z_right_bound = z_coordinate + .5
    x_left_bound = round(x_left_bound, 2)
    x_right_bound = round(x_right_bound, 2)
    z_left_bound = round(z_left_bound, 2)
    z_right_bound = round(z_right_bound, 2)
    # make sure our bounds do not overshoot, if they do, we can make them bigger on one side
    if x_left_bound < -1.00:
        x_left_bound = -1.00
        x_right_bound = 0.00
    if x_right_bound > 1.00:
        x_right_bound = 1.00
        x_left_bound = 0.00
    if z_left_bound < -1.00:
        z_left_bound = -1.00
        z_right_bound = 0.00
    if z_right_bound > 1.00:
        z_right_bound = 1.00
        z_left_bound = 0.00

    # this is what we will compare to as we iterate through to find the count of our region
    true_x_left_bound = -1.0
    true_x_right_bound = 0.0
    x_region_counter = 0
    true_z_left_bound = -1.0
    true_z_right_bound = 0.0
    z_region_counter = 0

    tol = 1e-9

    # find the region number where the bounds are the same
    while(True):
        if ((x_left_bound - true_x_left_bound) <= tol) and ((x_right_bound - true_x_right_bound) <= tol):
            break
        else:
            true_x_left_bound = true_x_left_bound + distance_threshold
            true_x_right_bound = true_x_right_bound + distance_threshold
            x_region_counter = x_region_counter + 1


    while(True):
        if ((z_left_bound - true_z_left_bound) <= tol) and ((z_right_bound - true_z_right_bound <= tol)):
            break
        else:
            true_z_left_bound = true_z_left_bound + distance_threshold
            true_z_right_bound = true_z_right_bound + distance_threshold
            z_region_counter = z_region_counter + 1

    # generate the advice
    advice_text = "XRegion " + str(x_region_counter) + " ZRegion " + str(z_region_counter) 

    return advice_text

def generateAdvice(advice_list, random_coordinate, training=True):
    '''generate an advice given the coordinate'''

    # upper left, lower left, upper right, lower right
    coordinate_string_dict = {0: "upper left", 1: "lower left", 2: "upper right", 3: "lower right"}
    #coordinate_string_dict_2 = {0: "northwestern region", 1: "southwestern region", 2:"northeastern region", 3: "southeastern region"}
    coordinate_string_dict_2 = {0: "north western region", 1: "south western region", 2:"north eastern region", 3: "south eastern region"}

    possible_dictionaries_list = [coordinate_string_dict, coordinate_string_dict_2]
    chosen_dictionary = random.choice(possible_dictionaries_list)
    
    quadrant_number = determineSpatial(random_coordinate[0], random_coordinate[2])

    advice_0 = advice_list[0] % ("x", "z", chosen_dictionary[quadrant_number])
    advice_1 = advice_list[1] % (chosen_dictionary[quadrant_number])
    advice_2 = advice_list[2] % (chosen_dictionary[quadrant_number])

    possible_advice_list = [advice_0, advice_1, advice_2]

    advice_text = random.choice(possible_advice_list)

    return advice_text

def tokenize_current_advice(chosen_advice, tokenized_advice_dict):
    '''given the tokens dictionary that the pre-trained model understands, tokenize this text advice. This advice would have been generated from the function above generateAdvice'''

    chosen_advice = chosen_advice.replace(",", "")

    advice_vector = np.zeros(FLAGS.maxadvicelength)
    advice_tokens = chosen_advice.split(" ")
    for i in range(len(advice_tokens)):
        advice_vector[i] = tokenized_advice_dict[advice_tokens[i]]

    return advice_vector, len(advice_tokens)

# load the data
for k in range(0, 3):
    df = pandas.DataFrame([line.strip().split() for line in open(fileList[k], 'r')])
    df.fillna("", inplace=True)
    data = np.array(df)

    # A minibatch consists of a target location, a world, 
    # and 9 sentences that share the same target/world.
    minibatches = []

    # go from 1 to d in steps of 9
    # 11870
    for i1 in range(0, data.shape[0] - 1, 9):
        # go through each individual one
        target = np.reshape(np.asarray(data[i1, range_start:range_end], dtype=np.float), (3, 1))
        world = np.reshape(np.asarray(data[i1,6:66], dtype=np.float), (3,20), order='F')
        sentences = []
        for i in range(i1, i1+9):
            # get in batches of 60
            #@assert target == np.reshape(np.asarray(data[i1, range_start:range_end], dtype=np.float32), (3, 1))
            # @assert world == np.reshape(np.asarray(data[i1,6:66]), (3,20))
            sent = []
            # maybe this is 67
            for j in range (66, len(data[2])):
                # if "", break, else add to the sentence
                if data[i, j] == "":
                    break
                sent.append(data[i,j])
                if int(data[i, j]) > FLAGS.xvocab:
                    if fileList[k] == FLAGS.train_file:
                        FLAGS.xvocab = int(data[i, j])
                    
            sentences.append(sent)

        minibatches.append((target, world, sentences))

    all_data.append(minibatches)

print("Initializing the model...")

FLAGS.xvocab = FLAGS.xvocab + 1

lastloss = bestloss = sys.maxint

# FLAGS.maxlength = 83 since we can possible have an 83 word sentence and must pass in one by one

input_data = tf.placeholder(tf.int32, [FLAGS.batch_size, FLAGS.maxlength], name="input_data")
# store how many sequences to go until we have to update the gradients
# one dimensional array
lengths = tf.placeholder(tf.int32, [FLAGS.batch_size], name="lengths")
# the advice text and it's lengths
advice_data = tf.placeholder(tf.int32, [FLAGS.batch_size, FLAGS.maxadvicelength], name="advice_data")
advice_lengths = tf.placeholder(tf.int32, [FLAGS.batch_size], name="advice_lengths")
# correct outputs is a matrix of the correct x,y,z coordinates for each element in the batch
labels = tf.placeholder(tf.float32, [3, FLAGS.batch_size], name="labels")

# Define embeddings matrix
with tf.variable_scope("embeddings"):
    embeddings = tf.Variable(tf.random_uniform([FLAGS.xvocab, FLAGS.word_embedding_size], -1, 1, seed=20160503))

with tf.name_scope("dropout_placeholer"):
    dropout_prob_placeholder = tf.placeholder_with_default(1.0, shape=())

# RNN architecture
multicells = 1

# the lstm cell
lstm = tf.contrib.rnn.LSTMCell(FLAGS.hidden_layer_size, state_is_tuple=True, initializer=tf.contrib.layers.xavier_initializer(seed=FLAGS.random_seed_value))
# dropout cell
lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=dropout_prob_placeholder)
# create a cell of the LSTM cell 
lstm = tf.contrib.rnn.MultiRNNCell(cells=[lstm] * multicells, state_is_tuple=True)

# we have two softmax, one of size FLAGS.nblocks, another of size FLAGS.ndirs
output_layer = {}
with tf.name_scope("output-20-weight"):
    output_layer[0] = Layer.W(1 * FLAGS.advice_hidden_layer_size, FLAGS.nblocks, 'OutputLayer')
    tf.summary.histogram("output-20_summ", output_layer[0])
with tf.name_scope("output-10-weight"):
    output_layer[1] = Layer.W(1 * FLAGS.advice_hidden_layer_size, FLAGS.ndirs, 'OutputLayer2')
    tf.summary.histogram("output-10_summ", output_layer[1])
# # add bias to them, not sure if needed
output_bias = {}
with tf.name_scope("output-20-bias"):
    output_bias[0] = Layer.b(FLAGS.nblocks, 'OutputBias')
with tf.name_scope("output-10-bias"):
    output_bias[1] = Layer.b(FLAGS.ndirs, 'OutputBias2')

def lstm_cell(hidden_size):
    lstmCell = tf.contrib.rnn.LSTMCell(hidden_size, state_is_tuple=True, initializer=tf.contrib.layers.xavier_initializer(seed=FLAGS.random_seed_value))
    lstmCell = tf.contrib.rnn.DropoutWrapper(lstmCell, output_keep_prob=dropout_prob_placeholder)
    return lstmCell

# lstm cell for the advice text
advice_LSTM = tf.contrib.rnn.MultiRNNCell([lstm_cell(size) for size in FLAGS.num_layers * [FLAGS.advice_hidden_layer_size]], state_is_tuple=True)

# make the RNN graph
# inputs
rnn_inputs = tf.nn.embedding_lookup(embeddings, input_data)

with tf.variable_scope("advice_embeddings"):
    advice_embeddings = tf.Variable(tf.random_uniform([200, FLAGS.advice_embedding_size], -1, -1, seed=20160503))
    advice_rnn_inputs = tf.nn.embedding_lookup(advice_embeddings, advice_data)

# process the original instruction through the LSTM
with tf.variable_scope("lstm0"):
    # create the rnn graph at run time
    # sequence length allows us to input variable lengths
    # tensorflow returns zero vectors for states and outputs only after the sequence length.
    outputs, fstate = tf.nn.dynamic_rnn(cell=lstm, inputs=rnn_inputs,
                                      sequence_length=lengths, 
                                      dtype=tf.float32, time_major=False)

# process the advice through the LSTM
with tf.variable_scope("advice_LSTM"):
    advice_outputs, advice_fstate = tf.nn.dynamic_rnn(cell=advice_LSTM, 
        inputs=advice_rnn_inputs, 
        sequence_length=advice_lengths, dtype=tf.float32, time_major=False)

def build_column(x, input_size, column_size):
    w = tf.Variable(tf.random_normal([input_size, column_size]))
    b = tf.Variable(tf.random_normal([column_size]))
    processing1 = tf.nn.relu(tf.matmul(x, w) + b)
    return processing1

# change the dimension of the advice LSTM output with some FC connected layers.
with tf.name_scope("lstm_output_layer"):
    lstm_output_layer = build_column(tf.concat([f.h for f in advice_fstate], 1), 256, FLAGS.fc_column_size)
with tf.name_scope("lstm_fc_layer"):
    lstm_fc_layer = tf.contrib.layers.fully_connected(lstm_output_layer, num_outputs=256, activation_fn=tf.nn.relu)
    tf.summary.histogram("lstm_fc_layer", lstm_fc_layer)

# sum up the instruction-understanding LSTM and the advice understanding FC connected layer. The rest of the model is the same as Bisk Et. al
logits = {}
with tf.name_scope("output1-20-compute"):
    logits[0] = tf.matmul((fstate[0].h + lstm_fc_layer), output_layer[0]) + output_bias[0]

with tf.name_scope("output2-9-compute"):
    logits[1] = tf.matmul((fstate[0].h + lstm_fc_layer), output_layer[1]) + output_bias[1]

# FLAGS.nblocks output
with tf.name_scope("softmax-20"):
    refblock = tf.nn.softmax(logits[0])
# FLAGS.ndirs output
with tf.name_scope("softmax-9"):
    direction = tf.nn.softmax(logits[1])


world_placeholder = tf.placeholder(tf.float32, [FLAGS.world_size, FLAGS.world_length], "world_data")
with tf.name_scope("world_fc"):
    world_fc = tf.contrib.layers.fully_connected(world_placeholder, num_outputs=20, 
    activation_fn=tf.nn.relu)

    # multiply the world by the softmax output of size FLAGS.nblocks (20)
    refxyz = tf.matmul(world_fc, tf.transpose(refblock))

#refxyz = tf.matmul(world_placeholder, tf.transpose(refblock))

# need another weight set to return something of size FLAGS.ndims (3)
# output_layer[0] = Layer.W(multicells * Fhidden_layer_size, FLAGS.nblocks, 'OutputLayer')
with tf.name_scope("Weights_9_to_3_Dims"):
    output_dimensions = Layer.W(FLAGS.ndirs, FLAGS.ndims, name='OffsetWeights')
    tf.summary.histogram("weights_9-3_summ", output_dimensions)
offset = tf.matmul(direction, output_dimensions)

# add these results together to get a matrix of size (3, 9)
with tf.name_scope("resulting_coordinate"):
    result = refxyz + tf.transpose(offset)    

# the variables to load later on
advice_saved_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     "advice_LSTM")
advice_saved_variables = advice_saved_variables + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "advice_embeddings")
advice_saved_variables = advice_saved_variables + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "lstm_output_layer")

# the variables to train on. everything advice related is frozen
general_training_variables = tf.all_variables()
general_training_variables = list(set(general_training_variables) - set(advice_saved_variables))

# Learning
with tf.name_scope("regular_optimizer"):
    loss = tf.reduce_mean(tf.squared_difference(result, labels))
    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)
    #gradients, variables = zip(*optimizer.compute_gradients(loss))
    gradients, variables = zip(*optimizer.compute_gradients(loss, var_list=general_training_variables))
    gradients, _ = tf.clip_by_global_norm(gradients, FLAGS.gradient_clip_threshold)
    optimize = optimizer.apply_gradients(zip(gradients, variables))

    tf.summary.scalar('loss_labels', loss)
    

with tf.name_scope("correct_prediction"):
    # distance from the true coordinates normalized by block length
    correct_prediction = tf.reduce_sum([(tf.sqrt(tf.reduce_sum([(result[:, j][i] - labels[:, j][i])**2 for i in range(3) ]))/0.1524) for j in range(FLAGS.batch_size)])/FLAGS.batch_size
    output_1 = ((tf.sqrt(tf.reduce_sum([(result[:, 0][i] - labels[:, 0][i])**2 for i in range(3) ]))/0.1524))
    output_2 = ((tf.sqrt(tf.reduce_sum([(result[:, 1][i] - labels[:, 1][i])**2 for i in range(3) ]))/0.1524))
    output_3 = ((tf.sqrt(tf.reduce_sum([(result[:, 2][i] - labels[:, 2][i])**2 for i in range(3) ]))/0.1524))
    output_4 = ((tf.sqrt(tf.reduce_sum([(result[:, 3][i] - labels[:, 3][i])**2 for i in range(3) ]))/0.1524))
    output_5 = ((tf.sqrt(tf.reduce_sum([(result[:, 4][i] - labels[:, 4][i])**2 for i in range(3) ]))/0.1524))
    output_6 = ((tf.sqrt(tf.reduce_sum([(result[:, 5][i] - labels[:, 5][i])**2 for i in range(3) ]))/0.1524))
    output_7 = ((tf.sqrt(tf.reduce_sum([(result[:, 6][i] - labels[:, 6][i])**2 for i in range(3) ]))/0.1524))
    output_8 = ((tf.sqrt(tf.reduce_sum([(result[:, 7][i] - labels[:, 7][i])**2 for i in range(3) ]))/0.1524))
    output_9 = ((tf.sqrt(tf.reduce_sum([(result[:, 8][i] - labels[:, 8][i])**2 for i in range(3) ]))/0.1524))

## Training
advice_saver = tf.train.Saver(advice_saved_variables)
saver = tf.train.Saver()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config)
session.run(tf.global_variables_initializer())

# restore the pre-trained model parameters
advice_saver.restore(session, FLAGS.trained_model_save_path)
#saver.restore(session, FLAGS.model_save_path)

# generate all the advice sentences so that we can tokenize them later 
def generate_advice_list(minibatches, batchsize, advice_list, self_generated=False, training=True):
    generated_advice_list = []

    # passing the data through the network
    for (target, world, sents) in minibatches:
        if len(sents) != batchsize:
            print("Bad length, error")

        for j in range(0, batchsize):

            # generate advice
            # the training flag can be useful if you want to have variability in text at test time.
            if self_generated:
                generated_advice = generateAdviceCenter(advice_list, target, training=training)
                generated_advice_list.append(generated_advice)
            else:
                generated_advice = generateAdvice(advice_list, target, training=training)
                generated_advice_list.append(generated_advice)
                
    return generated_advice_list

# train one set of minibatches
def trainModel(sess, minibatches, batchsize, generated_advice_list, advice_tokens_dictionary, epochsCount, training=True, testing=False):

    sumloss = numloss = 0
    y = np.zeros((3, batchsize), np.float)
    mask = np.zeros(batchsize, np.uint8)
    input_vector = np.zeros((batchsize, FLAGS.maxlength), np.int32)
    advice_input_vector = np.zeros((FLAGS.batch_size, FLAGS.maxadvicelength), np.int32)

    total_loss = 0.0
    predictions = []
    advice_counter = 0

    self_generated_advice_list = []
    self_generated_advice_accuracy_list = []

    # passing the data through the network
    for (target, world, sents) in minibatches:

        if len(sents) != batchsize:
            print("Bad length, error")

        # array to store the length of each sentence in the batch
        sequence_size_length = []
        for k in range(len(sents)):
            sequence_size_length.append(len(sents[k]))

        # add the instruciton
        input_vector[:] = 0
        for j in range(len(sents)):
            s = sents[j]
            for i in range(len(s)):
                input_vector[j, i] = s[i]
          
        # set up the label with the true coordinate  
        y[:] = 0
        y += target

        advice_size_length = []

        for j in range(0, len(input_vector)):

            # generate a advice region half the time during training stages
            # this teaches the model to handle situations both with and without advice
            if training == True:
                advice_gen_prob = np.random.randint(0, 2)
            else:
                advice_gen_prob = 1

            # if not training, always use advice
            if training == False:
                advice_gen_prob = 0

            # if generating self-generated advice, don't also feed in advice
            if FLAGS.generate_advice:
                advice_gen_prob = 1

            # generate advice
            if advice_gen_prob == 0:

                # tokenize advice
                current_advice, current_advice_length = tokenize_current_advice(generated_advice_list[advice_counter], advice_tokens_dictionary)

                advice_counter = advice_counter + 1
                advice_size_length.append(current_advice_length)

                # copy to the advice vector
                for i in range(len(current_advice)):
                    advice_input_vector[j, i] = current_advice[i]
            else:
                advice_size_length.append(0)
                advice_counter = advice_counter + 1

            
        # set up all the input data. add dropout if in training mdoe
        feed_dict = {input_data: input_vector, lengths: sequence_size_length, world_placeholder: world, labels: y, advice_lengths: advice_size_length, advice_data: advice_input_vector}

        if training == True:
            feed_dict = {input_data: input_vector, lengths: sequence_size_length, world_placeholder: world, labels: y, advice_lengths: advice_size_length, advice_data: advice_input_vector, dropout_prob_placeholder: FLAGS.dropout}

        advice_input_vector = np.zeros((batchsize, FLAGS.maxadvicelength), np.int32)

        if training == True:

            # run the optimizer
            _, current_loss = sess.run([optimize, loss], feed_dict=feed_dict)

            total_loss += current_loss

        else:

            # get the average error for all the examples
            resOutput1, resOutput2, resOutput3, resOutput4, resOutput5, resOutput6, resOutput7, resOutput8, resOutput9, made_prediction, resulting_values = sess.run([output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8, output_9, correct_prediction, result], feed_dict=feed_dict)
            resOutputTotal = [resOutput1, resOutput2, resOutput3, resOutput4, resOutput5, resOutput6, resOutput7, resOutput8, resOutput9]
            shouldRerunProgram = False

            # generate the input specific self generated advice for each prediction coordinate and store it
            if FLAGS.generate_advice:
                for curr_counter in range(FLAGS.batch_size):
                    x_coordinate = resulting_values[:, curr_counter][0]
                    z_coordinate = resulting_values[:, curr_counter][2]
                    # the 0 here is just a placeholder as generateAdviceCenter is designed to work both for this case and generating input-specific advice given an x,y, z coordinate. The function only considers x and z coordinates but, as those correspond to regions.
                    curr_advice = generateAdviceCenter(advice_list, [x_coordinate, 0, z_coordinate], training=training)
                    self_generated_advice_list.append(curr_advice)

                    # compute the accuracy of this input specific self-generated advice
                    self_generated_advice_accuracy_list.extend(calculate_accuracy_self_generated(resulting_values, target, FLAGS.batch_size))
            
            predictions.extend(resOutputTotal)

    if training == True:
        return np.mean(total_loss)
    else:
        return predictions, self_generated_advice_list, np.mean(self_generated_advice_accuracy_list)


# generate advice. pass in the flag for self_generated to know if we are generating input specific self generated advice or not
train_advice_list = generate_advice_list(all_data[0], FLAGS.batch_size, advice_list, self_generated=FLAGS.self_generated_advice, training=True)
validate_advice_list = generate_advice_list(all_data[1], FLAGS.batch_size, advice_list, self_generated=FLAGS.self_generated_advice, training=True)
test_advice_list = generate_advice_list(all_data[2], FLAGS.batch_size, advice_list, self_generated=FLAGS.self_generated_advice, training=False)
if FLAGS.self_generated_advice:
    # if we are doing input specific self generated advice, then the test advice must be self generated, so load it in
    test_advice_list = np.load(FLAGS.test_advice_save_file).tolist()


best_train_average = sys.maxint
best_test_average = sys.maxint
best_validation_average = sys.maxint
best_epoch = 0
best_test_median = sys.maxint

best_dev_self_gen_acc = 0
best_test_self_gen_acc = 0

# do the training and evaluation
for epoch in range (FLAGS.epochs):

    start_time = time.time()

    trainLoss = trainModel(session, all_data[0], FLAGS.batch_size, train_advice_list, advice_tokens_dictionary, epoch)
    print('Epoch %d: %f' % (epoch, trainLoss))
    predictions, _, _ = trainModel(session, all_data[0], FLAGS.batch_size, train_advice_list, advice_tokens_dictionary, epoch, training=False)
    predictionsTrainModel = predictions
    sum_values = sum(predictions)
    average0 = sum_values / len(predictions)
    best_train_average = min(best_train_average, average0)

    # training with the advice all the time, just like test time
    predictions, _, _ = trainModel(session, all_data[0], FLAGS.batch_size, train_advice_list, advice_tokens_dictionary, epoch, training=False)
    predictionsTrainModel = predictions
    sum_values = sum(predictions)
    average_train_advice = sum_values / len(predictions)

    predictions, _, dev_self_gen_acc = trainModel(session, all_data[1], FLAGS.batch_size, validate_advice_list, advice_tokens_dictionary, epoch, training=False)
    sum_values = sum(predictions)
    average1 = sum_values / len(predictions)
    best_validation_average = min(best_validation_average, average1)

    predictions, test_self_generated_advice_list, test_self_gen_acc = trainModel(session, all_data[2], FLAGS.batch_size, test_advice_list, advice_tokens_dictionary, epoch, training=False)
    sum_values = sum(predictions)
    average2 = sum_values / len(predictions)
    median2 = np.median(predictions)
    best_test_median = min(best_test_median, median2)

    if FLAGS.generate_advice:
        if dev_self_gen_acc > best_dev_self_gen_acc:
            print("Saving model at " + str(FLAGS.model_save_path))
            saver.save(session, FLAGS.model_save_path)
            best_epoch = epoch

            best_dev_self_gen_acc = dev_self_gen_acc
            best_test_self_gen_acc = test_self_gen_acc
            np.save(FLAGS.test_advice_save_file, test_self_generated_advice_list)
            print("Test self generated advice accuracy: " + str(test_self_gen_acc))

        print("Best test self gen accuracy: " + str(best_test_self_gen_acc))

    else:
        # best model so far
        if average2 < best_test_average:
            print("Saving model at " + str(FLAGS.model_save_path))
            saver.save(session, FLAGS.model_save_path)
            best_epoch = epoch


    best_test_average = min(best_test_average, average2)

    elapsed_time = time.time() - start_time

    print("")

    print("Train average:")
    print(average0)
    print(average_train_advice)
    print("Validation average:")
    print(average1)
    print("Test average:")
    print(average2)
    print("Test median:")
    print(median2)
    print("Best epoch: " + str(best_epoch))
    print("Best test average: " + str(best_test_average))
    print("Best test median: " + str(best_test_median))
    print("Elapsed time:")
    print(elapsed_time)
    print("")

print("Best train average: " + str(best_train_average))
print("Best validation_average: " + str(best_validation_average))
print("Best test average: " + str(best_test_average))
if FLAGS.generate_advice:
    print("Best test self gen accuracy: " + str(best_test_self_gen_acc))
